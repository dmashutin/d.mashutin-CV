**********************************
Encoding Unicode: UTF-8 vs. UTF-16
**********************************
Before Unicode
==============
ASCII and Text Encoding
-----------------------
A human perceives a text as an arrangement of letters. Spaces between letters let us detect words, and words have their specific meanings. Computers deal with texts in their own way. They treat letters, digits, punctuation marks and whitespaces as characters, and process them in the form of bits. A bit is a basic unit of information that shows a logical state and has one of two possible values. These values are commonly represented as ‚Äú0‚Äù and ‚Äú1‚Äù.  Here is the word ‚Äúhello‚Äù represented in 0‚Äôs and 1‚Äôs:

01101000 01100101 01101100 01101100 01101111

A character encoding standard, or simply **encoding**, allows all computers to record characters as bits and read them in the same way. An **encoding** is a set of rules for converting texts from one representation to another.

In 1963 the first version of *The American Standard Code for Information Interchange (ASCII)* was published. It described 7-bit codes for 128 characters including digits, lowercase and uppercase Latin letters, punctuation symbols, non-printing characters, and control codes. Here are the letters of ‚Äúword‚Äù with their ASCII codes in binary:

+---+----------+
| w | 01000001 |
+---+----------+
| o | 01000010 |
+---+----------+
| r | 01000011 |
+---+----------+
| d | 01000100 |
+---+----------+

A **code page** is a table where characters are assigned to specific bit sequences. The table above is a simplified example of a code page.

Although ASCII in usually associated with binary representation, a bit sequence can be represented in any numerical system. Here are the ASCII codes of "hello‚Äù in hexadecimal:

68 65 6C 6C 6F

In ASCII each character is coded by 7 bits, while a byte consists of 8 bits. It means that the left-most bit is always ‚Äú0‚Äù. Making the left-most bit meaningful allows to double the number of available codes. And that was done to include letters of languages other than English. But 256 codes were still not enough for characters of all existing languages, so numerous code pages for different languages and their combinations appeared.

Single-Byte Character Encodings
-------------------------------
Most of such code pages use exactly on byte to encode each graphic character. They are commonly referred to as **single-byte character set (SBCS)** encodings. A **character set** or **charset** is a set of characters that can be encoded. E.g., the charset of ASCII is limited to 128 characters, while single-byte charsets can include up to 256 characters. To make the terms clear:

* An encoding is a set of rules for text conversion
* A code page is a table that assigns codes to characters
* A character set is a set of characters that can be encoded

These three terms are very often used interchangeably. But it is important to keep in mind their exact definitions for a better understanding of text conversion procedures.

Single-byte character sets were in common use in 1980‚Äôs and 1990‚Äôs, and their support is still available in Windows and other platforms. SBCS encodings (e.g., Windows-1252, KOI8-R, ISO/IEC 8859, etc.) are extended ASCII code pages as their first 128 codes are similar to ASCII. They are also referred to as ‚ÄúWindows code pages‚Äù. Here are the common issues and limitations of these encodings:

* They encode only 256 symbols including 128 codes reserved for ASCII characters
* System fonts must support a specific encoding
* Converting one encoding into another causes partial losses 
* Specific fonts must be provided when exchanging files between different platforms
* Asian languages with thousands of characters cannot be supported

Two and More Bytes Per Character
--------------------------------
One byte is not enough to encode texts in languages that use more than 256 unique characters. It is the case for most Asian languages, including Chinese, Japanese, Korean etc. Two bytes allow to assign codes for 65,536 different characters. **Double-byte character sets** (e.g., BIG-5, Shift JIS) use 16 bits (2 bytes) for each character. There can be several encodings for one language with different character sets and encoding methods. For example, BIG-5 covers mostly traditional Chinese, while GB18030 supports both Traditional Chinese and Simplified Chinese ideographs.

There are also **triple-byte charsets (TBCS)** and **multi-byte charsets (MBCS)** with three and more bytes per character.

The Unicode Standard
====================
That is how a plenty of national encodings came up. To make things worse, if someone needed to mix, for example, Slovenian and Swedish in the same text, he had no choice but to create a new encoding. That resulted in countless encodings. It was not possible to read a text file unless the original encoding was known and present in the system.

The global development of World Wide Web made it evident that people all over the world want to communicate in their national languages. The text data in all these languages must be properly encoded, transmitted and decoded by various software, systems and devices all over the world.

In 1987 a group of enthusiasts started work on an international and multilingual character encoding system. The idea was to provide codes for all characters of all living languages, create a unified table, and develop universal encoding/decoding methods.

In 1991 the Unicode Consortium published the first version of Unicode. At that time, the standard used a 16-bit design. Such a choice was based on the assumption that 2 bytes is enough to encode all characters in modern use, while all others do not need to be included.

Since 1996, when Unicode 2.0 came up, the character set is not limited to 16 bits. That allowed to include historical scripts, rare characters, emojis and a plenty of other symbols.

Version 13.0 was released in March 2020 adding 4,969 CJK (Chinese-Japanese-Korean) unified ideographs, some graphic characters, 55 emojis, etc.

Basics of Unicode
-----------------
First of all, Unicode is NOT an encoding. Unicode is a standard that includes character encodings along with code charts, reference data files, character properties and rules. The current version of the standard supports the **Unicode Transformation Format (UTF)** encodings: UTF-8, UTF-16, and UTF-32. The first two are the most commonly used ones.
Unicode defines a **codespace** of numerical values ranging from 0 to 10FFFF called **code points**. Each code point is denoted as ‚ÄòU+‚Äô plus the code point value in hexadecimal. The numeric value must be at least 4 digits long. For smaller numbers leading zeros must be added. E.g. a Latin small letter ‚Äòx‚Äô takes the 78th position in Unicode chart and is denoted as U+0078.

The Unicode codespace is divided into 17 **planes**, numbered 0 to 16. Each plane consists of 65,536 (216) code points. Plane 0 is called **Basic Multilingual Plane (BMP)** and contains the most commonly used characters. All Unicode planes can accommodate 1,114,112 code points. Besides the current repertoire of 143,859 characters they contain 2,048 surrogates (the range from U+D800 to U+DFFF), 66 non-characters, and 137,468 private-use characters. There are also 830,606 reserved code points, which are not assigned but available for use.

It is also essential to distinguish between the following concepts:

* a user-perceived character
* a code point assigned to a character by Unicode
* a sequence of bytes used to store the code of a character in memory

Check out the following table:


+------------------------------+---------------+------------+-------------+-----------+
| Description                  | Visual output | Code point | Sequence of bytes       |
+==============================+===============+============+=============+===========+
|                              |               |            | **UTF-8**   | **UTF-16**|
+------------------------------+---------------+------------+-------------+-----------+
| SMILING FACE WITH SUNGLASSES | üòé            | U+1F60E    | f0 9f 98 8e | d83d de0e |
+------------------------------+---------------+------------+-------------+-----------+
| ARABIC LETTER HAMZA          | ÿ°             | U+0621     | d8 a1       | 0621      |
+------------------------------+---------------+------------+-------------+-----------+
| LATIN SMALL LETTER I         | i             | U+0069     | 69          | 0069      |
+------------------------------+---------------+------------+-------------+-----------+

The table clearly shows that the sequence of bytes depends on the encoding.

UTF-8
-----
The name of this encoding derives from Unicode Transformation Format ‚Äì 8-bit.  8-bit refers to the length of each code unit. A code unit is the unit of storage of a code point. A code point can be encoded with one or several code units. UTF-8 is a variable length character encoding. It uses one code unit for each of the first 128 code points, and up to 4 code units for each of the subsequent ones. In other words, a Unicode character in UTF-8 occupies from 1 to 4 bytes of disk space.
A character can be encoded with 1, 2, 3, or 4 code units. The number of code units or bytes in each specific encoded character can be detected by one or several highest bits as follows:
Unicode code points	Bit mask in UTF-8
00-7F (7 bits)	0xxxxxxx
0080-07FF (11 bits)	110xxxxx 10xxxxxx
0800-FFFF (16 bits)	1110xxxx 10xxxxxx 10xxxxxx
010000-10FFFF (21 bits)	11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
This is how UTF-8 encodes U+0621 (ARABIC LETTER HAMZA) into ‚Äúd8 a1‚Äù:
1.	0621 belongs to the range 0080-07FF, so it will take 11 bits and two code units to encode it
2.	Convert ‚Äú0621‚Äù into binary -> 11000100001.
3.	Apply the 10xxxxxx mask to the bits 0-6 (0 is the least significant/the right-most bit) to get the second byte -> 10100001
4.	Apply the 110xxxxx mask to the remaining five bits to get the first byte -> 11011000
5.	Convert 11011000 10100001 into hexadecimal -> d8 a1.
The table above clearly shows that UTF-8 is a self-synchronizing encoding. A byte of the 10xxxxxx form is called a continuation byte. You can start decoding from any point ‚Äì just skip over continuation bytes until it is a non-continuation one. That will be the beginning of a byte sequence of a specific code point.
The first 128 code points of Unicode are similar to ASCII characters and include Latin letters, punctuation marks, digits and special characters. Each of them is encoded in UTF-8 with a single byte. Which means that HTML markup, CSS, URLs, etc. are encoded in the most efficient way. That significantly decreases the size of a webpage.
Those 128 code points are encoded with the same byte sequences as in ASCII and cannot be decoded wrong. Thus, UTF-8 is completely safe to use with programming and mark-up languages that process some ASCII symbols in a specific way.
UTF-16
The code unit in UTF-16 encoding is 16 bits long. It is also a variable length encoding, but unlike UTF-8 there are either 2 or 4 bytes per character. UTF-16 is commonly used for text strings in Microsoft Windows, Java, C#, and some other applications. 
One or two 16-bit code units in UTF-16 allow to encode the whole Unicode codespace. UTF-16 encodes each code point in the range U+0000-U+FFFF with a single byte. This range covers the characters of the most common languages and writing systems.
The code points in the range from U+10000 to U+10FFFF are encoded by UTF-16 in the form of surrogate pairs. A surrogate pair is two 16-bit code units. According to the Unicode standard, the code points within the range U+D800‚Äì¬¨U+DFFF are excluded from BMP and used to create surrogate pairs in UTF-16.
This is how UTF-16 creates a surrogate pair to encode Unicode Han Character U+22023:
1.	10000 is subtracted from the code point:
22023 ‚Äì 10000 = 12023 (hexadecimal) ‚Äì> 0001001000 0000100011 (binary, 20 bits)

2.	The high ten bits are added to D800:
0001001000 (binary) ‚Äì> 48 (hexadecimal) + D800 = D848
That is the first 16-bit code unit or high surrogate

3.	The low ten bits are added to DC00:
0000100011 (binary) ‚Äì> 23 (hexadecimal) + DC00= DC23
That is the second 16-bit code unit or low surrogate

4.	The high surrogate and the low surrogate form the surrogate pair:
d848 dc23
The ranges of high surrogates and low surrogates are excluded from the BMP. The first byte of a surrogate pair can never be decoded as a BMP code point. On the other hand, two consecutive code points can never be taken for a surrogate pair.
Check out the decoding procedure for d803 de62:
1.	D803 belongs to the range D800-DBFF
DE62 belongs to the range DC00-DFFF
It is definitely a surrogate pair, not two consecutive code points

2.	Check the value of the bit 9 to decide if the surrogate is high/low (counting from the right, the right-most bit is the bit 0):

D803 ‚Äì> 1101100000000011 ‚Äì the bit 9 is 0, it is a high surrogate

DE62 ‚Äì> 1101111001100010 ‚Äì the bit 9 is 1, it is a low surrogate

3.	The high six bits are meaningless ‚Äì they only indicate that it is a surrogate. Remove them from each surrogate and make up a 20-bit sequence:
0000000011 1001100010 ‚Äì> e62

4	Add 10000 in hexadecimal to get the code point in Unicode:
E62 + 10000 = 10E62
U+10E62 ‚Äì RUMI DIGIT THREE

Byte Order in UTF-16
The encoded characters in the examples above are represented as sequences of bytes. But a computer stores such sequences in memory and reads them in a specific way which depends on the endianness of the particular system architecture. There are big-endian (BE) and little-endian (LE) systems.
Big-endian systems store the most significant byte at the smallest memory address. The clearest example is how we write down Arabic numerals. This byte order is a standard for network protocols, as well as for IBM, SPARC and Motorola processors. It is sometimes referred to as network byte order or Motorola byte order. A big-endian system will store d848 dc23 as ‚Äúd8 48 dc 23‚Äù.
Little-endian architecture, in contrast, stores the least significant byte at the smallest address. Like as if we wrote down an Arabic numeral 123 as ‚Äú321‚Äù. Most x86 processors use this byte order and it is often called Intel byte order. A little-endian system will store d848 dc23 as ‚Äú48 d8 23 dc‚Äù.
In UTF-16 the byte order is specified by a Byte Order Mark (BOM). BOM is an U+FEFF code point recorded as the first character in text. In Unicode U+FEFF stands for the invisible zero-width non-breaking space (ZWNBSP). If the first byte is read as U+FEFF, it means that the decoder has the same architecture as the encoder and the whole text must be decoded regularly. But if it is U+FFEF, which is a non-character in Unicode, bytes must be swapped in the remaining code units.
 The byte order can also be designated by the name of the encoding. If the encoding is specified as UTF-16LE or UTF-16BE, it means that the byte order is little-endian or big-endian respectively. In this case a BOM must not be present and U+FEFF must be decoded as ZWNBSP. However, most applications still ignore U+FEFF when it is the first character in a text.
UTF-8 vs. UTF-16 in Software Development
To reveal the pros and cons of these two encodings in a more explicit way, let us consider them with regard to a specific software development environment.
Microsoft Windows-based architecture
Win32 API, which is common for modern editions of Microsoft Windows, supports two methods of text representation: traditional 8-bit Windows code pages and UTF-16. The reasons for that are rather historical than objective. Windows 10 supports UTF-8 as a code page, but its internal encoding is UTF-16LE.
In order to interoperate with Windows APIs, you may convert between UTF-8 and UTF-16 by using  MultiByteToWideChar and WideCharToMultiByte functions. Starting from Windows Version 1903 (May 2019 Update) Microsoft recommends using UTF-8 character encoding for web applications to ensure optimal compatibility, minimize localization bugs, and reduce testing costs.
Web and Linux
In Web and Linux environments UTF-8 is commonly used and treated as the best encoding for Unicode. C++ code, HTML\XML tags, filesystem paths, variables and parameters consist predominantly of ASCII characters, which is one byte per character in UTF-8. That provides for efficient traffic and memory usage and improves the overall performance significantly. UTF-8 totally dominates the World Wide Web. Over 95% of websites in 2020 use this encoding. For certain languages this value is close to 100%.
There is a popular misconception that UTF-8 is only more efficient for English texts. It is true that UTF-16 could save up to 50% for an abstract Chinese text compared to UTF-8, as each code point would take two bytes instead of four. But in real world plain texts rarely exist independently without any meta-data, which is efficiently encoded by UTF-8. The actual gain of using UTF-16 will be far less than 50%, while highly probable compatibility issues will definitely ruin the perceived advantage.
Conclusion
Unicode is a globally recognized standard. It is unreasonable and virtually impossible to ignore it in software development. Text strings used internally within each separate application may have any format. The only condition is proper support of the whole Unicode codespace and lossless data exchange with external modules or APIs. 
When it comes to the choice of a specific Unicode encoding, it is essential to consider all relevant factors. These are system architecture requirements, use cases, customer requirements, software development workflow, etc. The decision must be taken at a very early stage and implemented by all contributors to the development process.
